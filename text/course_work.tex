\documentclass[bachelor, och, coursework]{SCWorks}
% параметр - тип обучения - одно из значений:
%    spec     - специальность
%    bachelor - бакалавриат (по умолчанию)
%    master   - магистратура
% параметр - форма обучения - одно из значений:
%    och   - очное (по умолчанию)
%    zaoch - заочное
% параметр - тип работы - одно из значений:
%    referat    - реферат
%    coursework - курсовая работа (по умолчанию)
%    diploma    - дипломная работа
%    pract      - отчет по практике
% параметр - включение шрифта
%    times    - включение шрифта Times New Roman (если установлен)
%               по умолчанию выключен
\usepackage{subfigure}
\usepackage{tikz,pgfplots}
\pgfplotsset{compat=1.5}
\usepackage{float}

%\usepackage{titlesec}
\setcounter{secnumdepth}{4}
%\titleformat{\paragraph}
%{\normalfont\normalsize}{\theparagraph}{1em}{}
%\titlespacing*{\paragraph}
%{35.5pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}

\titleformat{\paragraph}[block]
{\hspace{1.25cm}\normalfont}
{\theparagraph}{1ex}{}
\titlespacing{\paragraph}
{0cm}{2ex plus 1ex minus .2ex}{.4ex plus.2ex}

% --------------------------------------------------------------------------%

\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\graphicspath{ {./images/} }
\usepackage{tempora}

\usepackage[sort,compress]{cite}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{fancyvrb}
\usepackage{listings}
\usepackage{listingsutf8}
\usepackage{longtable}
\usepackage{array}
\usepackage[english,russian]{babel}

% \usepackage[colorlinks=true]{hyperref}
\usepackage{url}

\usepackage{underscore}
\usepackage{setspace}
\usepackage{indentfirst} 
\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{enumitem}
\usepackage{tikz}
\usepackage{minted}

\setminted[py]{fontsize=\small, breaklines=true, style=bw, linenos}

\newcommand{\eqdef}{\stackrel {\rm def}{=}}
\newcommand{\specialcell}[2][c]{%
\begin{tabular}[#1]{@{}c@{}}#2\end{tabular}}

\renewcommand\theFancyVerbLine{\small\arabic{FancyVerbLine}}

\newtheorem{lem}{Лемма}

\begin{document}

% Кафедра (в родительном падеже)
\chair{теоретических основ компьютерной безопасности и криптографии}

% Тема работы
\title{Анализ тональности отзывов о фильмах с помощью алгоритмов машинного обучения}

% Курс
\course{4}

% Группа
\group{431}

% Факультет (в родительном падеже) (по умолчанию "факультета КНиИТ")
\department{факультета КНиИТ}

% Специальность/направление код - наименование
%\napravlenie{09.03.04 "--- Программная инженерия}
%\napravlenie{010500 "--- Математическое обеспечение и администрирование информационных систем}
%\napravlenie{230100 "--- Информатика и вычислительная техника}
%\napravlenie{231000 "--- Программная инженерия}
\napravlenie{100501 "--- Компьютерная безопасность}

% Для студентки. Для работы студента следующая команда не нужна.
% \studenttitle{Студентки}

% Фамилия, имя, отчество в родительном падеже
\author{Улитина Ивана Владимировича}

% Заведующий кафедрой
\chtitle{} % степень, звание
\chname{Абросимов М. Б.}

%Научный руководитель (для реферата преподаватель проверяющий работу)
\satitle{доцент} %должность, степень, звание
\saname{Слеповичев И. И.}

% Руководитель практики от организации (только для практики,
% для остальных типов работ не используется)
% \patitle{к.ф.-м.н.}
% \paname{С.~В.~Миронов}

% Семестр (только для практики, для остальных
% типов работ не используется)
%\term{8}

% Наименование практики (только для практики, для остальных
% типов работ не используется)
%\practtype{преддипломная}

% Продолжительность практики (количество недель) (только для практики,
% для остальных типов работ не используется)
%\duration{4}

% Даты начала и окончания практики (только для практики, для остальных
% типов работ не используется)
%\practStart{30.04.2019}
%\practFinish{27.05.2019}

% Год выполнения отчета
\date{2023}

\maketitle

% Включение нумерации рисунков, формул и таблиц по разделам
% (по умолчанию - нумерация сквозная)
% (допускается оба вида нумерации)
% \secNumbering

%-------------------------------------------------------------------------------------------

\tableofcontents

\intro

    В течении последних нескольких лет одним из актуальных направлений
    искусственного интеллекта является обработка и генерация текста. Технологии
    в рамках этой сферы машинного и глубокого обучения постоянно развиваются и
    незамедлительно находят применение в человеческом обиходе. Примерами
    применения результатов изучения алгоритмов в этой области являются
    суммаризаторы и классификаторы текста, различные голосовые помощники или
    чат-боты с искусственным интеллектом. Последние, в свою очередь, получили
    широкое распространение из-за удобства их использования при решении самых
    разных задач "--- от генерации ими рецептов различных блюд или анекдотов, до
    генерации рабочего и компилирующегося кода, который выполняет некоторую
    описанную пользователем функцию.

    Одной из классических задач этой области искусственного интеллекта считается
    анализ тональности, суть которого состоит в том, чтобы при некотором входном
    тексте сделать вывод о том, какой эмоциональный окрас имеет этот текст
    (например, анализ текста комментария пользователя на сайте для просмотра
    фильмов, отражающий впечатления человека относительно просмотренного кино).
    Такая задача распространена и её решение может входить в основу различных
    рекомендательных систем, статистических сводок относительно продаваемой
    продукции или других аспектов маркетинга.

    Целью данной курсовой работы является построение системы анализа тональности
    отзывов о фильмах на английском языке. В рамках теоретической части данной
    курсовой работы будут рассматриваться алгоритмы машинного обучения,
    применяемые при решении поставленной задачи, а также способы обработки
    используемого набора данных и методы оценки качества системы. На основе
    проделанной работы будут сформулированы выводы о различных способах решения
    проблемы.

\defabbr

    \textit{Искусственный интеллект} (англ. Artificial Intelligence) "---
    технология создания алгоритмов, лежащих в основе проектирования
    интеллектуальных машин и программ, способных имитировать деятельность
    человека.

    \textit{Нейронная сеть (нейросеть)} (англ. Neural Network) "---
    математическая модель, чаще всего имеющая программную интерпретацию, сутью
    которой является реализация деятельности, похожей на деятельность
    биологических нейронных сетей. Нейронная сеть используется при создании
    какого-либо из алгоритмов искусственного интеллекта и состоит из
    совокупности нейронов, соединенных между собой связями. 

    \textit{Обработка текста на естественном языке} (англ. Natural Language\\
    Processing, NLP) "--- одно из направлений развития машинного обучения,
    искусственного интеллекта и науки о данных, а также математической
    лингвистики. В данной области знаний рассматриваются проблемы компьютерного
    анализа и преобразования текстов на языках, используемых людьми для общения.


\section{Теоретическая часть}

    \subsection{Способы предобработки текста}

        Важным этапом решения задачи применения алгоритмов машинного обучения
        является первичная обработка (предобработка) данных, которые в
        дальнейшем будут использоваться алгоритмами для обучения и на основе
        содержимого которых будут функционировать построенные предиктивные
        системы. Чем более качественная выборка используется для задачи, тем
        лучше будут результаты работы таких алгоритмов.

        В зависимости от типа данных для обучения (изображения, текст, числовые
        значения), используются соответствующие методы обработки выборки.
        Описанные ниже способы предобработки будут применяться в дальнейшем при
        написании практической части курсовой работы.

        \subsubsection{Мешок слов}

            % переформулировать текст ниже
            Проблема текстов заключается в том, что они беспорядочны и могут
            иметь разную длину, а большинство алгоритмов машинного обучения
            предполагают входные и выходные параметры фиксированной длины.

            Исходя из этого, алгоритмы машинного обучения не могут работать
            напрямую с необработанным текстом: его необходимо преобразовать в
            последовательности чисел (векторы). При языковой обработке векторы
            формируются из текстовых данных, отражая лингвистические и
            статистические свойства текста. Это называется извлечением или
            кодированием признаков.
            
            Мешок слов (англ. Bag-of-Words, BoW) "--- один из таких методов
            обработки. Это представление текста в виде мультимножества без учета
            его грамматических особенностей и порядка слов, которое описывает
            информацию об их количестве в этом тексте. Подход очень прост и
            гибок, его можно использовать множеством способов для извлечения
            характеристик текста. В частности, практическая часть в общем виде
            подразумевает следующий порядок действий:

            \begin{enumerate}
                \item Удаление из текста знаков пунктуации, спецсимволов
                (различных скобок и т.п.).
                \item Перевод текста в нижний регистр (в силу отсутствия
                необходимости знания о порядке слов).
                \item Преобразование каждого текста в список из слов.
                \item Создание словаря – списка уникальных слов, присутствующих
                во всех кодируемых текстах, где каждому слову будет
                соответствовать некоторое число.
                \item Замена списка слов на векторы, состоящие из чисел, которые
                этим словам соответствуют (токенизация).
                \item Формирование на основе векторов матрицы-результата, в
                которой для каждого слова (столбца) и каждого текста (строки)
                соответствует количество использования этого слова в этом
                тексте.

            \end{enumerate}

            Это называется ''мешком'' слов, потому что всякая информация о
            порядке или структуре слов в документе отбрасывается. Полученная
            структура в первую очередь предназначена для хранения информации о
            частоте использования слова в тексте, а не об их порядке.

        \subsubsection{Коллокации}

            Существует несколько способов улучшить применение мешка слов по
            отношению к тексту, и часть этих способов образуется путем удаления
            из текстов мало информативных конструкций. Помимо удаления
            пунктуации, сокращений, стоп-слов и замены больших букв, можно также
            использовать $n$-граммы.
            
            Как уже ранее упоминалось, при токенизации одно слово заменяется на
            одно числовое значение. $n$-граммой в данном случае называется
            токен, определяющий совокупность из $n$ слов. Таким образом, можно
            выделить биграммы, триграммы и т.д.

            Используя $n$-граммы в качестве токенов, возникает проблема высокой
            размерности результирующей матрицы, так как все пары слов
            значительно увеличивают длину словаря. В качестве уменьшения
            размерности могут использоваться различные способы удаления не
            слишком информативных $n$-грамм (например, удаление биграмм,
            содержащих междометия, частицы, артикли).

            Информативные $n$-граммы называются \textbf{коллокациями}. Для того,
            чтобы в обрабатываемом тексте оставались исключительно коллокации,
            можно:

            \begin{enumerate}
                \item удалить часто встречающиеся $n$-граммы (те же артикли,
                которые в английском языке широко распространены в текстах). Чем
                чаще встречается некоторая $n$-грамма, тем меньше конкретной
                информации оно содержит и, как следствие, будет слабо
                охарактеризовывать некоторый текст;
                \item удалить слишком редко встречающиеся $n$-граммы (это могут
                быть опечатки, слишком специфичная лексика);
            \end{enumerate}

        \subsubsection{Частота слова и обратная частота документа}

            С помощью частоты слова (англ. term frequency, TF) можно оценить то,
            насколько часто встречаются токены/слова в конкретном документе.
            Значимость некоторого слова пропорциональна частоте использования
            этого слова в тексте и обратно пропорциональна частоте использования
            слова во всех текста выборки. Частоту слова можно определить
            следующей формулой:

            $$tf(t, d) = \frac{n_t}{\sum_k n_k},$$

            где $n_t$ "--- число вхождений слова $t$ в текст, а $\sum_k n_k$
            "--- общее число слов в данном тексте.

            Обратная частота документа (англ. inverse document frequency, IDF)
            "--- это инверсия частоты слова, встречающегося во всех документах
            выборки. Она определяет, насколько часто слова появляются во всех
            остальных документах. Для каждого уникального токена/слова в
            пределах одной выборки документов существует только одно значение
            IDF.

            $$idf(t, D) = \log \frac{|D|}{|\{d_i \in D | t \in d_i \}|},$$

            где $|D|$ "--- число документов в выборке, $|\{d_i \in D | t \in d_i
            \}|$ "--- число документов из выборки $D$, в которых встречается
            слово $t$ (при $n_t \neq 0$).

            Значение основания логарифма в формуле не имеет существенной
            важности в силу того, что его изменение может привести только к
            изменению веса каждого токена/слова на постоянный множитель, но это
            не влияет на соотношение весов между собой.

            С помощью этих двух статистических мер может быть сформирована ещё
            одна мера (TF-IDF), которая выглядит следующим образом:

            $$tf \text{-} idf(t, d, D) = tf(t,d) \times idf(t, D)$$

            Большую значимость в TF-IDF получат токены/слова с высокой частотой
            в рамках конкретного текста и с низкой частотой упоминаний в других
            документах.

        \subsubsection{Стемминг и лемматизация}
            Стеммингом (англ. stemming) называется преобразование слова, после
            которого от него остается только корень. Это своего рода
            нормализация слов. Стемминг важен тогда, когда при формировании
            мешка слов обнаруживается большое количество однокоренных слов.
            Например, слова ''wait'', ''waiting'', ''waited'', ''waits'' имеют
            схожую смысловую нагрузку, однако в мешке слов будут представлять
            собой разные сущности, что будет способствовать ухудшению работы
            алгоритма машинного обучения. Проще вместо четырех однокоренных слов
            оставить один термин "--- ''wait'', на которое и будет заменены все
            вариации этого слова.

            Лемматизация – это процесс определения леммы слова исходя из его
            значения. Лемматизацию относят к морфологическому анализу слов,
            задачей которого является удаление флективных окончаний (тех, что
            соотносятся по значению с корнем). Это способствует преобразованию
            слова в свою базовую или словарную форму, которое также называется
            леммой.

            Применение стемминга и лемматизации к тексту способствует сокращению
            размера формирующегося мешка слов. Это вызвано тем, что приведение
            различных форм слова к одной единственной, а также удаление
            флективных окончаний уменьшает количество разнообразных слов в
            наборе данных, что приводит к повышению качества работы алгоритмов
            машинного обучения за счет однозначности кодирования разных форм
            слов, имеющих один и тот же смысл.
            

    \subsection{Алгоритмы машинного обучения}
        \subsubsection{Полиномиальный наивный байесовский классификатор}
            
            % https://biconsult.ru/products/polinomialnyy-naivnyy-bayesovskiy-algoritm-v-mashinnom-obuchenii

            Полиномиальный наивный байесовский алгоритм – одна из разновидностей
            наивного байесовского алгоритма в машинном обучении, который очень
            полезен для использования в наборе данных, который распределяется
            полиномиально. Когда есть несколько классов для классификации, может
            использоваться именно этот алгоритм, потому что для прогнозирования
            метки текста он вычисляет вероятность каждой метки для входного
            текста, а затем генерирует метку с наибольшей вероятностью в
            качестве выходных данных.

            Вот некоторые из преимуществ использования этого алгоритма для
            полиномиальной классификации:

                \begin{enumerate}
                    \item Легко использовать для непрерывных и дискретных
                    данных.
                    \item Может обрабатывать большие наборы данных.
                    \item Может классифицировать данные по нескольким меткам.
                    \item Лучше всего использовать для обучения моделей
                    обработки естественного языка.
                \end{enumerate}

            % https://scikit-learn.org/stable/modules/naive_bayes.html
            MultinomialNB реализует наивный байесовский алгоритм для
            полиномиально распределенных данных и является одним из двух
            классических наивных байесовских вариантов, используемых в
            классификации текста (где данные обычно представлены как счетчики
            векторов слов, хотя векторы tf-idf также хорошо работают на
            практике). Распределение параметризуется векторами $\theta_y =
            (\theta_{y_1}, \dots, \theta_{y_n})$ для каждого класса $y$, где $n$
            "--- количество функций (в классификации текста "--- размер
            словарного запаса) и $\theta_{y_i}$ это вероятность $P(x_i | y)$
            особенности $i$ входящие в выборку, принадлежащую к классу $y$.

            Параметры $\theta_y$ оценивается сглаженной версией максимального
            правдоподобия, то есть подсчетом относительной частоты:
            $$\hat{\theta}{y_i} = \frac{N_{y_i} + \alpha}{N_y + \alpha n}$$

            где $N_{y_i} = \sum_{x \in T} x_i$ это количество признака $i$,
            появляющееся в объекте класса $y$ в обучающем наборе $T$, и $N_y =
            \sum_{i = 1}^{n} N_{y_i}$ это общее число всех признаков для класса
            $y$.

            Сглаживающие приоры $\alpha \geq 0$ учитывает признаки/особенности,
            отсутствующие в обучающих выборках, и предотвращает нулевые
            вероятности в дальнейших вычислениях. Установка парамера $\alpha =
            1$ называется сглаживанием Лапласа, а $\alpha < 1$ называется
            сглаживанием Лидстоуна.

        \subsubsection{Метод опорных векторов}

            % https://scikit-learn.org/stable/modules/svm.html

            

        \subsubsection{Логистическая регрессия}

    \subsection{Метрики оценки качества обучения}


\section{Практическая часть}

    \subsection{Описание инструментов и библиотек программной реализации}
    \subsection{Описание набора данных для обучения и теста}
    \subsection{Условия проведения обучения}
    \subsection{Результаты обучения}

\conclusion

\begin{thebibliography}{99}
    \bibitem{neur} Короткий С., ''Нейронные сети: Основные положения'',
    [Электронный ресурс] : [статья] / URL:
    http://www.shestopaloff.ca/kyriako/Russian/Artificial_Intelligence/Some_publications/Korotky_Neuron_network_Lectures.pdf
    (дата обращения 27.04.2022) Загл. с экрана. Яз. рус.
    
    \bibitem{Gud} Гудфеллоу Я., Бенджио И., Курвилль А., ''Глубокое обучение'',
    г. Москва, Издательство ДМК, 2018 г., Яз. рус.
    
    \bibitem{dataset3} ADITYAJN105, ''Flickr 8k Dataset '', [Электронный ресурс]
    : [статья] / URL https://www.kaggle.com/datasets/adityajn105/flickr8k (дата
    обращения 8.05.2022) Загл. с экрана. Яз. англ.

    % model

    %architecture pic
    % http://shikib.com/captioning.html
    
    \bibitem{architecture} Mehri S., ''Image Captioning'', [Электронный ресурс]
    : [статья] / URL http://shikib.com/captioning.html (дата обращения
    14.04.2022) Загл. с экрана. Яз. англ.

    \bibitem{inception} ''Inception_v3'', [Электронный ресурс] : [статья] / URL
    https://pytorch.org/hub/pytorch_vision_inception_v3/ (дата обращения
    8.05.2022) Загл. с экрана. Яз. англ.

    \bibitem{fwpandas} ''pandas'' [Электронный ресурс] : [сайт] / URL:
    https://pandas.pydata.org/ (дата обращения 17.05.2022) Загл. с экрана. Яз.
    англ.

\end{thebibliography}

\appendix

    % \section{Код \texttt{getloader.py}}
    % \inputminted{py}{model-ver-2/getloader.py}

    % \section{Код \texttt{model.py}}
    % \inputminted{py}{model-ver-2/model.py}

    % \section{Код \texttt{train.py}}
    % \inputminted{py}{model-ver-2/train.py}

    % \section{Код \texttt{checkmetric.py}}
    % \inputminted{py}{model-ver-2/checkmetric.py}

\end{document}
